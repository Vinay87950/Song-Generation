import pandas as pd
import numpy as np
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
from bertopic import BERTopic
import hdbscan
import umap
import warnings
warnings.filterwarnings('ignore')

def load_medqa_dataset():
    """Load the MedQA-USMLE dataset and convert to pandas DataFrame"""
    dataset = load_dataset("GBaker/MedQA-USMLE-4-options")
    df = pd.DataFrame(dataset["train"])
    print(f"Loaded dataset with {len(df)} questions")
    return df

def create_combined_text(df):
    """Combine question with metamap phrases for better context"""
    # Check if metamap_phrases exists, otherwise use just the question
    if 'metamap_phrases' in df.columns:
        df['combined_text'] = df['question'] + " " + df['metamap_phrases'].fillna('')
    else:
        df['combined_text'] = df['question']
    return df

def generate_embeddings(texts, model_name="pritamdeka/BioBERT-base-finetuned-PubMedQA"):
    """Generate embeddings using a biomedical language model"""
    print(f"Generating embeddings using {model_name}...")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(texts, show_progress_bar=True)
    return embeddings

def cluster_questions(embeddings, n_neighbors=15, min_cluster_size=10):
    """Perform dimensionality reduction and clustering"""
    print("Reducing dimensions with UMAP...")
    umap_embeddings = umap.UMAP(
        n_neighbors=n_neighbors,
        min_dist=0.0,
        metric='cosine',
        random_state=42
    ).fit_transform(embeddings)
    
    print("Clustering with HDBSCAN...")
    clusters = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        metric='euclidean',
        gen_min_span_tree=True,
        prediction_data=True
    ).fit(umap_embeddings)
    
    return umap_embeddings, clusters.labels_

def apply_bertopic(df, embeddings):
    """Use BERTopic to automatically extract topics from clusters"""
    print("Extracting topics with BERTopic...")
    
    # Configure BERTopic
    topic_model = BERTopic(
        embedding_model=None,  # We've already created embeddings
        nr_topics="auto",
        verbose=True
    )
    
    # Fit the model with pre-computed embeddings
    topics, probs = topic_model.fit_transform(df['combined_text'].tolist(), embeddings)
    
    # Get topic info
    topic_info = topic_model.get_topic_info()
    
    # Get representative terms for each topic
    topic_terms = {}
    for topic in topic_info['Topic'].values:
        if topic != -1:  # Skip outlier topic
            topic_terms[topic] = topic_model.get_topic(topic)
    
    return topics, topic_info, topic_terms

def visualize_clusters(umap_embeddings, cluster_labels, save_path="cluster_visualization.png"):
    """Create a visualization of the clusters"""
    plt.figure(figsize=(12, 10))
    plt.scatter(
        umap_embeddings[:, 0], 
        umap_embeddings[:, 1], 
        c=cluster_labels, 
        cmap='hsv', 
        s=5, 
        alpha=0.8
    )
    plt.colorbar(label='Cluster')
    plt.title('Question Clusters')
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"Visualization saved to {save_path}")

def analyze_clusters(df, cluster_labels, topic_terms=None):
    """Analyze clusters to understand categorization"""
    df['cluster'] = cluster_labels
    
    # Count documents per cluster
    cluster_counts = df['cluster'].value_counts().sort_index()
    print("\nCluster distribution:")
    print(cluster_counts)
    
    # Save categorized data
    df_categorized = df[['question', 'metamap_phrases', 'cluster']]
    df_categorized.to_csv('categorized_questions.csv', index=False)
    print("Saved categorized questions to categorized_questions.csv")
    
    # If we have topic terms, create a mapping file
    if topic_terms:
        topic_mapping = []
        for topic, terms in topic_terms.items():
            # Create a description based on top terms
            description = ", ".join([term[0] for term in terms[:5]])
            topic_mapping.append({
                'cluster_id': topic,
                'top_terms': description,
                'question_count': len(df[df['cluster'] == topic])
            })
        
        pd.DataFrame(topic_mapping).to_csv('cluster_descriptions.csv', index=False)
        print("Saved cluster descriptions to cluster_descriptions.csv")
    
    return df_categorized

def main():
    # Load dataset
    df = load_medqa_dataset()
    
    # Prepare text
    df = create_combined_text(df)
    
    # Generate embeddings
    embeddings = generate_embeddings(df['combined_text'].tolist())
    
    # Cluster the questions
    umap_embeddings, cluster_labels = cluster_questions(embeddings)
    
    # Extract topics
    topics, topic_info, topic_terms = apply_bertopic(df, embeddings)
    
    # Visualize results
    visualize_clusters(umap_embeddings, cluster_labels)
    
    # Analyze and save results
    df_categorized = analyze_clusters(df, topics, topic_terms)
    
    print("\nCategorization complete!")
    print(f"Found {len(set(topics)) - (1 if -1 in topics else 0)} distinct question categories")

if __name__ == "__main__":
    main()
